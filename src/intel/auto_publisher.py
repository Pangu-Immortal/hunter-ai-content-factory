"""
Hunter AI å†…å®¹å·¥å‚ - å…¨èƒ½çŒæ‰‹æ¨¡å—

åŠŸèƒ½ï¼š
- ç»¼åˆæ‰«æ HackerNews + Twitter + Reddit + GitHub Trending + å°çº¢ä¹¦
- å›´ç»• AI ç”Ÿæˆå·¥å…·çš„ç‘•ç–µå’Œçƒ­ç‚¹è¿›è¡Œåˆ†æ
- ç”Ÿæˆã€ŒAI ç”Ÿæ´»é»‘å®¢ã€é£æ ¼çš„è§£å†³æ–¹æ¡ˆæ–‡ç« 
- ä¿å­˜ MD æŠ¥å‘Šåˆ°æ•°æ®åº“
- æ¨é€åˆ°å¾®ä¿¡

ä½¿ç”¨æ–¹æ³•ï¼š
    uv run python -m src.intel.auto_publisher

GitHub: https://github.com/Pangu-Immortal/hunter-ai-content-factory
Author: Pangu-Immortal
"""

import asyncio
import datetime
import random
import time

from rich.console import Console
from rich.progress import track
from twikit import Client as TwitterClient

from src.config import settings
from src.intel.utils import (
    create_article_dir,
    create_http_client,
    generate_content_id,
    get_article_file_path,
    get_chromadb_client,
    get_today_str,
    push_to_wechat,
)
from src.utils.ai_client import get_ai_client

# ç»ˆç«¯è¾“å‡ºç¾åŒ–
console = Console()

# Twitter çŒæ€å…³é”®è¯ï¼ˆå¤šæ¨¡æ€å…¨ç”Ÿæ€ç‰ˆï¼‰
TWITTER_KEYWORDS = [
    # å›¾åƒç”Ÿæˆ
    "Midjourney hands weird",
    "Flux text spelling error",
    "AI art consistent character",
    "DALL-E ugly face",
    "Stable Diffusion color dull",
    "AI background messy",
    # è§†é¢‘ç”Ÿæˆ
    "Runway morphing weird",
    "Luma physics fail",
    "Kling video flickering",
    "AI video face melting",
    "Sora movement unnatural",
    # éŸ³é¢‘/éŸ³ä¹
    "Suno lyrics wrong",
    "Udio robotic voice",
    "AI music ending abrupt",
    "ElevenLabs emotionless",
    "AI voice clone glitch",
    # æ–‡æœ¬å†™ä½œ
    "ChatGPT sounds like AI",
    "Claude too formal",
    "DeepSeek hallucination",
    "LLM repetitive phrases",
    "AI essay lack of depth",
    "marketing copy boring",
]

# åƒåœ¾è¯é»‘åå•
SPAM_FILTERS = [
    "100+ AI Tools",
    "Check my bio",
    "Sign up now",
    "Top 10 tools",
    "Affiliate",
    "Crypto",
    "Giveaway",
    "NFT",
]

# Hacker News è¿‡æ»¤é—¨æ§›
HN_MIN_SCORE = 100


class AutoPublisher:
    """å…¨èƒ½çŒæ‰‹ - ç»¼åˆä¿¡æ¯é‡‡é›†ä¸æ–‡ç« ç”Ÿæˆ"""

    def __init__(self):
        """åˆå§‹åŒ–å…¨èƒ½çŒæ‰‹"""
        self.intel_list: list[str] = []  # æœ¬æ¬¡ä¼šè¯æƒ…æŠ¥åˆ—è¡¨
        self.intel_images: list[str] = []  # é‡‡é›†çš„å›¾ç‰‡ URL åˆ—è¡¨
        self.intel_sources: list[dict] = []  # æƒ…æŠ¥æºè¯¦æƒ…ï¼ˆåŒ…å«å›¾ç‰‡ï¼‰
        self.article_content: str = ""  # ç”Ÿæˆçš„æ–‡ç« å†…å®¹
        self.article_title: str = ""  # æ–‡ç« æ ‡é¢˜
        self.push_status: str = ""  # æ¨é€çŠ¶æ€
        self.http = create_http_client(timeout=15.0)
        self._init_ai_client()
        self._init_chromadb()

    def _init_ai_client(self):
        """åˆå§‹åŒ– AI å®¢æˆ·ç«¯"""
        if not settings.gemini.api_key:
            console.print("[red]âŒ AI API Key æœªé…ç½®[/red]")
            raise ValueError("AI API Key æœªé…ç½®")

        self.ai_client = get_ai_client()
        provider = "ç¬¬ä¸‰æ–¹èšåˆ" if settings.gemini.is_openai_compatible else "å®˜æ–¹ Gemini"
        console.print(f"[green]âœ… AI å®¢æˆ·ç«¯è¿æ¥æˆåŠŸ ({provider})[/green]")

    def _init_chromadb(self):
        """åˆå§‹åŒ– ChromaDB"""
        client = get_chromadb_client()
        self.collection = client.get_or_create_collection(name="market_insights")
        console.print("[green]âœ… ChromaDB æ•°æ®åº“è¿æ¥æˆåŠŸ[/green]")

    def is_spam(self, text: str) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦ä¸ºåƒåœ¾ä¿¡æ¯

        Args:
            text: æ–‡æœ¬å†…å®¹

        Returns:
            bool: æ˜¯å¦ä¸ºåƒåœ¾ä¿¡æ¯
        """
        return any(spam.lower() in text.lower() for spam in SPAM_FILTERS)

    def save_and_buffer(
        self, source: str, author: str, content: str, tag: str, images: list[str] = None, url: str = ""
    ) -> bool:
        """
        ä¿å­˜æƒ…æŠ¥å¹¶åŠ å…¥ç¼“å†²åŒº

        Args:
            source: æ¥æº
            author: ä½œè€…
            content: å†…å®¹
            tag: æ ‡ç­¾
            images: å›¾ç‰‡ URL åˆ—è¡¨
            url: åŸå§‹é“¾æ¥

        Returns:
            bool: æ˜¯å¦ä¸ºæ–°æƒ…æŠ¥
        """
        images = images or []
        try:
            doc_id = generate_content_id(source, content, str(author))

            # æŸ¥é‡
            existing = self.collection.get(ids=[doc_id])
            if existing and existing["ids"]:
                console.print(f"  ğŸ’¤ [è·³è¿‡æ—§é—»] {content[:20]}...")
                return False

            current_time = datetime.datetime.now().isoformat()
            self.collection.upsert(
                documents=[content],
                metadatas=[{"source": source, "author": str(author), "tag": str(tag), "time": current_time}],
                ids=[doc_id],
            )

            intel_item = f"ã€{source}ã€‘({tag}) @{author}: {content}"
            self.intel_list.append(intel_item)

            # ä¿å­˜å›¾ç‰‡å’Œæƒ…æŠ¥æºè¯¦æƒ…
            if images:
                self.intel_images.extend(images)
            self.intel_sources.append(
                {
                    "source": source,
                    "author": author,
                    "content": content[:100],
                    "tag": tag,
                    "url": url,
                    "images": images,
                }
            )

            console.print(f"  ğŸ’¾ [æ•è·æ–°çŸ¥] {content[:30]}...")
            return True

        except Exception as e:
            console.print(f"  [red]âš ï¸ å­˜å‚¨è·³è¿‡: {e}[/red]")
            return False

    def hunt_hacker_news(self) -> int:
        """
        æ‰«æ Hacker News çƒ­é—¨

        Returns:
            int: æ•è·æ•°é‡
        """
        console.print("\n[bold cyan]ğŸ”¥ [1/5] æ‰«æ Hacker News...[/bold cyan]")
        count = 0

        try:
            response = self.http.get("https://hacker-news.firebaseio.com/v0/topstories.json")
            top_ids = response.json()[:15]  # å–å‰ 15 æ¡

            for item_id in track(top_ids, description="HN æ–‡ç« "):
                try:
                    item = self.http.get(f"https://hacker-news.firebaseio.com/v0/item/{item_id}.json").json()

                    if item and item.get("score", 0) >= HN_MIN_SCORE:
                        title = item.get("title")
                        item_url = item.get("url", "")
                        hn_link = f"https://news.ycombinator.com/item?id={item_id}"
                        content = f"Title: {title} | Link: {item_url}"
                        # HackerNews æ— å›¾ç‰‡ï¼Œä½†ä¿å­˜é“¾æ¥
                        if self.save_and_buffer("HackerNews", "Tech", content, "Trend", url=hn_link):
                            count += 1

                    time.sleep(0.5)

                except Exception:
                    pass

        except Exception as e:
            console.print(f"[red]âŒ HN æ¨¡å—æŠ¥é”™: {e}[/red]")

        return count

    async def hunt_twitter(self) -> int:
        """
        æ‰«æ Twitterï¼ˆå…¨ç”Ÿæ€éšæœºï¼‰

        Returns:
            int: æ•è·æ•°é‡
        """
        import json

        console.print("\n[bold cyan]ğŸ¦ [2/5] æ‰«æ Twitter...[/bold cyan]")
        client = TwitterClient(language="en-US")
        count = 0

        # éšæœºæŠ½å– 6 ä¸ªå…³é”®è¯
        daily_keywords = random.sample(TWITTER_KEYWORDS, min(6, len(TWITTER_KEYWORDS)))
        console.print(f"ğŸ¯ ä»Šæ—¥ç‹©çŒç›®æ ‡: {daily_keywords}")

        cookies_file = settings.twitter.cookies_file
        if not cookies_file.exists():
            console.print(f"[red]âŒ Twitter Cookies æ–‡ä»¶ä¸å­˜åœ¨: {cookies_file}[/red]")
            return 0

        try:
            # åŠ è½½å¹¶è½¬æ¢ cookies æ ¼å¼
            with open(cookies_file, encoding="utf-8") as f:
                cookies_data = json.load(f)

            # æ£€æŸ¥æ ¼å¼å¹¶è½¬æ¢
            if isinstance(cookies_data, list):
                # Cookie-Editor æ•°ç»„æ ¼å¼ â†’ å­—å…¸æ ¼å¼
                cookies_dict = {c["name"]: c["value"] for c in cookies_data if "name" in c and "value" in c}
                console.print(f"[dim]ğŸ”„ å·²è½¬æ¢ {len(cookies_dict)} ä¸ª cookies ä¸º twikit æ ¼å¼[/dim]")
                client.set_cookies(cookies_dict)
            elif isinstance(cookies_data, dict):
                # å·²ç»æ˜¯å­—å…¸æ ¼å¼ï¼Œç›´æ¥ä½¿ç”¨
                client.set_cookies(cookies_data)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„ cookies æ ¼å¼: {type(cookies_data)}")

            for keyword in daily_keywords:
                try:
                    console.print(f"  ğŸ” æœç´¢: {keyword}")
                    tweets = await client.search_tweet(keyword, product="Latest", count=3)

                    if not tweets:
                        console.print("     (æ— æ–°å†…å®¹)")
                        continue

                    for tweet in tweets:
                        text = tweet.text.replace("\n", " ")
                        if self.is_spam(text):
                            continue

                        user = tweet.user.name if tweet.user else "Unknown"

                        # æå–æ¨æ–‡åª’ä½“å›¾ç‰‡
                        images = []
                        if hasattr(tweet, "media") and tweet.media:
                            for media in tweet.media:
                                if hasattr(media, "media_url_https"):
                                    images.append(media.media_url_https)
                                elif isinstance(media, dict) and media.get("media_url_https"):
                                    images.append(media["media_url_https"])

                        # æ„å»ºæ¨æ–‡é“¾æ¥
                        tweet_url = ""
                        if tweet.user and hasattr(tweet.user, "screen_name"):
                            tweet_url = f"https://twitter.com/{tweet.user.screen_name}/status/{tweet.id}"

                        if self.save_and_buffer("Twitter", user, text, keyword, images=images, url=tweet_url):
                            count += 1

                    await asyncio.sleep(2)

                except Exception as e:
                    console.print(f"     [yellow]âš ï¸ è·³è¿‡: {e}[/yellow]")

        except Exception as e:
            console.print(f"[red]âŒ Twitter æ¨¡å—æŠ¥é”™: {e}[/red]")

        return count

    async def hunt_reddit(self) -> int:
        """
        æ‰«æ Reddit çƒ­é—¨ AI è®¨è®º

        Returns:
            int: æ•è·æ•°é‡
        """
        console.print("\n[bold cyan]ğŸ”´ [3/5] æ‰«æ Reddit...[/bold cyan]")
        count = 0

        try:
            from src.intel.reddit_hunter import RedditHunter

            hunter = RedditHunter(mode="trending")
            await hunter.run()

            for post in hunter.posts[:10]:  # å–å‰ 10 æ¡
                content = f"Title: {post.title}"
                if post.selftext:
                    content += f" | Content: {post.selftext[:200]}"

                # æå–å¸–å­ç¼©ç•¥å›¾
                images = [post.thumbnail] if post.thumbnail else []
                if self.save_and_buffer(
                    "Reddit", f"r/{post.subreddit}", content, "AI Discussion", images=images, url=post.permalink
                ):
                    count += 1

            console.print(f"[green]âœ… Reddit é‡‡é›†: {count} æ¡[/green]")

        except ImportError as e:
            console.print(f"[yellow]âš ï¸ Reddit æ¨¡å—ä¸å¯ç”¨: {e}[/yellow]")

        except Exception as e:
            console.print(f"[red]âŒ Reddit æ¨¡å—æŠ¥é”™: {e}[/red]")

        return count

    async def hunt_github_trending(self) -> int:
        """
        æ‰«æ GitHub Trending AI é¡¹ç›®

        Returns:
            int: æ•è·æ•°é‡
        """
        console.print("\n[bold cyan]ğŸ™ [4/5] æ‰«æ GitHub Trending...[/bold cyan]")
        count = 0

        try:
            from src.intel.github_trending import GitHubTrendingHunter

            hunter = GitHubTrendingHunter()
            projects = await hunter.fetch_trending(since="daily")

            for project in projects[:10]:  # å–å‰ 10 ä¸ª
                content = f"Project: {project.name} | Stars: {project.stars} | {project.description[:100]}"

                # ä½¿ç”¨ Socialify æœåŠ¡ç”Ÿæˆé¡¹ç›®å¡ç‰‡å›¾
                socialify_url = (
                    f"https://socialify.git.ci/{project.name}/image"
                    f"?description=1&font=Inter&language=1&name=1&owner=1"
                    f"&pattern=Plus&stargazers=1&theme=Auto"
                )

                if self.save_and_buffer(
                    "GitHub", project.name, content, "Trending", images=[socialify_url], url=project.url
                ):
                    count += 1

            console.print(f"[green]âœ… GitHub Trending é‡‡é›†: {count} æ¡[/green]")

        except ImportError as e:
            console.print(f"[yellow]âš ï¸ GitHub Trending æ¨¡å—ä¸å¯ç”¨: {e}[/yellow]")

        except Exception as e:
            console.print(f"[red]âŒ GitHub Trending æ¨¡å—æŠ¥é”™: {e}[/red]")

        return count

    async def hunt_xiaohongshu(self) -> int:
        """
        æ‰«æå°çº¢ä¹¦ AI ç›¸å…³å†…å®¹

        Returns:
            int: æ•è·æ•°é‡
        """
        console.print("\n[bold cyan]ğŸ“• [5/5] æ‰«æå°çº¢ä¹¦...[/bold cyan]")
        count = 0

        try:
            from src.intel.xiaohongshu_browser import XiaohongshuBrowser

            hunter = XiaohongshuBrowser()

            if not hunter.is_logged_in():
                console.print("[yellow]âš ï¸ å°çº¢ä¹¦æœªç™»å½•ï¼Œè·³è¿‡[/yellow]")
                return 0

            # æœç´¢ AI ç›¸å…³å†…å®¹
            notes = await hunter.search(keyword="AIå·¥å…·", count=5)

            for note in notes:
                # æ”¯æŒ XhsNote å¯¹è±¡å’Œå­—å…¸ä¸¤ç§æ ¼å¼
                if hasattr(note, "title"):
                    # XhsNote å¯¹è±¡
                    title = note.title
                    desc = note.desc[:100] if note.desc else ""
                    author = note.author
                    images = note.images
                    url = note.url or f"https://www.xiaohongshu.com/explore/{note.note_id}"
                else:
                    # å­—å…¸æ ¼å¼
                    title = note.get("title", "")
                    desc = note.get("desc", "")[:100]
                    author = note.get("author", "å°çº¢ä¹¦ç”¨æˆ·")
                    images = note.get("images", [])
                    url = note.get("url", "")

                content = f"Title: {title} | Desc: {desc}"

                if self.save_and_buffer("å°çº¢ä¹¦", author, content, "AIå·¥å…·", images=images, url=url):
                    count += 1

            console.print(f"[green]âœ… å°çº¢ä¹¦é‡‡é›†: {count} æ¡[/green]")

        except ImportError as e:
            console.print(f"[yellow]âš ï¸ å°çº¢ä¹¦æ¨¡å—ä¸å¯ç”¨: {e}[/yellow]")

        except Exception as e:
            console.print(f"[yellow]âš ï¸ å°çº¢ä¹¦é‡‡é›†å¤±è´¥: {e}[/yellow]")

        return count

    def write_article(self, raw_data: str) -> str:
        """
        ä½¿ç”¨ Gemini ç”Ÿæˆæ–‡ç« 

        Args:
            raw_data: åŸå§‹æƒ…æŠ¥æ•°æ®

        Returns:
            str: ç”Ÿæˆçš„æ–‡ç« 
        """
        console.print("\n[bold cyan]âœï¸ AI æ­£åœ¨åˆ›ä½œ (æ¨¡å¼: AI ç”Ÿæ´»é»‘å®¢)...[/bold cyan]")

        prompt = f"""
        # Role: AI Lifehacker (AI ç”Ÿæ´»é»‘å®¢)
        # Mission: å¸®åŠ©æ™®é€šç”¨æˆ·è§£å†³ AI ç”Ÿæˆå†…å®¹ï¼ˆå›¾ã€æ–‡ã€éŸ³ã€å½±ï¼‰ä¸­çš„å…·ä½“ç‘•ç–µã€‚

        # Task
        åŸºäºã€Input Dataã€‘ä¸­çš„ç”¨æˆ·åæ§½ï¼Œå†™ä¸€ç¯‡è§£å†³æ–¹æ¡ˆæ–‡ç« ã€‚

        # æ ¸å¿ƒå·®å¼‚åŒ–ç­–ç•¥ (Differentiation)
        * **æ‹’ç»ç¨‹åºå‘˜è§†è§’**: ä¸è¦å†™ "Pythonä»£ç " æˆ– "APIè°ƒç”¨"ã€‚
        * **æ‹¥æŠ±åˆ›ä½œè€…è§†è§’**: ä½ çš„å—ä¼—æ˜¯å†™å°çº¢ä¹¦çš„ã€åšè§†é¢‘çš„ã€ç© AI ç»˜ç”»çš„ã€‚
        * **å¤šæ¨¡æ€äº¤ä»˜**:
            - å¦‚æœæ˜¯ç”»å›¾é—®é¢˜ï¼Œäº¤ä»˜ **Negative Prompt** æˆ– **æ„å›¾æŒ‡ä»¤**ã€‚
            - å¦‚æœæ˜¯è§†é¢‘é—®é¢˜ï¼Œäº¤ä»˜ **ç‰©ç†è§„å¾‹çº¦æŸè¯**ã€‚
            - å¦‚æœæ˜¯å†™ä½œé—®é¢˜ï¼Œäº¤ä»˜ **é£æ ¼è¿ç§» Prompt**ã€‚

        # Article Structure
        1. **ğŸ’” å´©æºƒç¬é—´ (The Fail)**:
           - ç”ŸåŠ¨æè¿°ç”¨æˆ·é‡åˆ°çš„é‚£ä¸ª"é¬¼ç•œ"ç¬é—´ã€‚
           - è¯æœ¯: "ä½ æ˜¯ä¸æ˜¯ä¹Ÿé‡åˆ°è¿‡è¿™ç§'äººå·¥æ™ºéšœ'æ—¶åˆ»ï¼Ÿ"

        2. **ğŸ”§ é­”æ³•ä¿®è¡¥ (The Fix)**:
           - ç”¨é€šä¿—çš„è¯­è¨€è§£é‡Šä¸ºä»€ä¹ˆ AI ä¼šçŠ¯é”™ã€‚
           - ç»™å‡ºè§£å†³æ–¹æ¡ˆã€‚

        3. **ğŸ å’’è¯­äº¤ä»˜ (The Spell)**:
           - **è¿™æ˜¯æ ¸å¿ƒï¼** æä¾›ä¸€æ®µå¯ç›´æ¥å¤åˆ¶çš„æŒ‡ä»¤ã€‚
           - æ ¼å¼å¿…é¡»æ˜¯ Markdown ä»£ç å—ã€‚
           - æ ‡æ³¨: "å¤åˆ¶è¿™æ®µå’’è¯­"ã€‚

        # Input Data
        {raw_data}
        """

        max_retries = 5
        wait_time = 5

        for attempt in range(max_retries):
            try:
                response = self.ai_client.generate_sync(prompt)
                return response.text

            except Exception as e:
                console.print(f"[yellow]âš ï¸ å†™ä½œå°è¯• {attempt + 1} å¤±è´¥: {e}[/yellow]")
                time.sleep(wait_time)
                wait_time += 5

        return "âŒ å†™ä½œå½»åº•å¤±è´¥"

    def deliver_result(self, article_content: str):
        """
        ç”Ÿæˆæ–‡æ¡£å¹¶æ¨é€

        Args:
            article_content: æ–‡ç« å†…å®¹
        """
        import json

        if article_content.startswith("âŒ"):
            return

        today = get_today_str()

        # æå–æ ‡é¢˜
        first_line = article_content.split("\n")[0].replace("#", "").strip()
        title = first_line[:30] if first_line else f"åˆ›æ„æ–¹æ¡ˆ_{today}"

        # åˆ›å»ºæ–‡ç« ä¸“å±ç›®å½•
        article_dir = create_article_dir(title)

        # ä¿å­˜ Markdown æ–‡ä»¶
        md_path = get_article_file_path(article_dir, "article.md")
        md_path.write_text(article_content, encoding="utf-8")
        console.print(f"[green]ğŸ“ Markdown å·²ä¿å­˜: {md_path}[/green]")

        # ä¿å­˜å…ƒæ•°æ®
        metadata = {
            "title": title,
            "date": today,
            "source": "auto_publisher",
            "intel_count": len(self.intel_list),
            "cover_images": self.intel_images[:10],  # ä¿ç•™å‰ 10 å¼ å›¾ç‰‡ä½œä¸ºå°é¢å€™é€‰
            "intel_sources": self.intel_sources,  # åŒ…å«å›¾ç‰‡çš„æƒ…æŠ¥æºè¯¦æƒ…
        }
        metadata_path = get_article_file_path(article_dir, "metadata.json")
        metadata_path.write_text(json.dumps(metadata, ensure_ascii=False, indent=2), encoding="utf-8")
        console.print(f"[green]ğŸ“‹ å…ƒæ•°æ®å·²ä¿å­˜: {metadata_path}[/green]")

        # æ¨é€åˆ°å¾®ä¿¡
        wechat_body = f"## ğŸ¨ {today} AI åˆ›æ„æ€¥æ•‘åŒ…\n\n{article_content}"
        push_to_wechat(title=f"ã€åˆ›æ„ã€‘{title}", content=wechat_body)

    async def run(self, platforms: list[str] = None):
        """
        è¿è¡Œå…¨èƒ½çŒæ‰‹å®Œæ•´æµç¨‹

        Args:
            platforms: æŒ‡å®šè¦é‡‡é›†çš„å¹³å°åˆ—è¡¨ï¼Œé»˜è®¤ä¸ºå…¨éƒ¨
                      å¯é€‰: ["hackernews", "twitter", "reddit", "github", "xiaohongshu"]
        """
        all_platforms = ["hackernews", "twitter", "reddit", "github", "xiaohongshu"]
        platforms = platforms or all_platforms

        counts = {}

        # HackerNews
        if "hackernews" in platforms:
            counts["hackernews"] = self.hunt_hacker_news()

        # Twitter
        if "twitter" in platforms:
            counts["twitter"] = await self.hunt_twitter()

        # Reddit
        if "reddit" in platforms:
            counts["reddit"] = await self.hunt_reddit()

        # GitHub Trending
        if "github" in platforms:
            counts["github"] = await self.hunt_github_trending()

        # å°çº¢ä¹¦
        if "xiaohongshu" in platforms:
            counts["xiaohongshu"] = await self.hunt_xiaohongshu()

        total = sum(counts.values())
        detail = " | ".join([f"{k}: {v}" for k, v in counts.items() if v > 0])
        console.print(f"\nğŸ“Š æ–°æƒ…æŠ¥æ€»é‡: {total} æ¡ ({detail})")

        if total > 0:
            raw_intel = "\n".join(self.intel_list)
            article = self.write_article(raw_intel)

            # ä¿å­˜æ–‡ç« å†…å®¹å’Œæ ‡é¢˜åˆ°å®ä¾‹å±æ€§
            self.article_content = article
            if not article.startswith("âŒ"):
                first_line = article.split("\n")[0].replace("#", "").strip()
                self.article_title = first_line[:30] if first_line else f"åˆ›æ„æ–¹æ¡ˆ_{get_today_str()}"

            self.deliver_result(article)

            # ä¿å­˜ MD æŠ¥å‘Šåˆ°æ•°æ®åº“
            self._save_report_to_db(article)

            self.push_status = "å·²æ¨é€" if settings.push.enabled else "æœªæ¨é€"
        else:
            console.print("[yellow]âŒ ä»Šæ—¥æœªå‘ç°æ–°æƒ…æŠ¥ï¼Œè·³è¿‡å†™ä½œ[/yellow]")
            self.push_status = "æ— å†…å®¹"

        self.http.close()

    def _save_report_to_db(self, content: str):
        """
        ä¿å­˜æŠ¥å‘Šåˆ° ChromaDB æ•°æ®åº“

        Args:
            content: æŠ¥å‘Šå†…å®¹
        """
        try:
            today = get_today_str()
            report_id = f"news_report_{today}"
            self.collection.upsert(
                documents=[content],
                metadatas=[
                    {
                        "type": "news_report",
                        "date": today,
                        "source": "auto_publisher",
                        "intel_count": len(self.intel_list),
                    }
                ],
                ids=[report_id],
            )
            console.print("[green]ğŸ’¾ æŠ¥å‘Šå·²å­˜å…¥æ•°æ®åº“[/green]")
        except Exception as e:
            console.print(f"[yellow]âš ï¸ æ•°æ®åº“å­˜å‚¨å¤±è´¥: {e}[/yellow]")


async def main():
    """ä¸»å‡½æ•°å…¥å£"""
    console.print("[bold magenta]ğŸš€ å…¨èƒ½çŒæ‰‹ v3.0 (5å¹³å°å…¨ç”Ÿæ€ç‰ˆ) å¯åŠ¨[/bold magenta]\n")

    try:
        publisher = AutoPublisher()
        await publisher.run()
    except KeyboardInterrupt:
        console.print("\n[yellow]âš ï¸ ç”¨æˆ·ä¸­æ–­[/yellow]")
    except Exception as e:
        console.print(f"[red]âŒ è¿è¡Œå¤±è´¥: {e}[/red]")
        raise


if __name__ == "__main__":
    asyncio.run(main())
