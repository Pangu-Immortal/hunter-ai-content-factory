"""
Hunter AI å†…å®¹å·¥å‚ - ç—›ç‚¹é›·è¾¾æ¨¡å—

åŠŸèƒ½ï¼š
- æ‰«æ Twitter + Reddit ä¸Šå…³äº AI äº§å“çš„ç”¨æˆ·æŠ±æ€¨
- ä½¿ç”¨ Gemini è¿›è¡Œç—›ç‚¹è¯Šæ–­åˆ†æ
- æ‰€æœ‰ç—›ç‚¹å­˜å…¥ SQLite æ•°æ®åº“ï¼ˆæ”¯æŒæ ‡ç­¾ã€åˆå¹¶ï¼‰
- ç”Ÿæˆ MD è¯Šæ–­æŠ¥å‘Šå¹¶å­˜å…¥æ•°æ®åº“
- æ¨é€åˆ°å¾®ä¿¡

ä½¿ç”¨æ–¹æ³•ï¼š
    uv run python -m src.intel.pain_radar

GitHub: https://github.com/Pangu-Immortal/hunter-ai-content-factory
Author: Pangu-Immortal
"""

import asyncio
import datetime
import random
import time

from rich.console import Console
from twikit import Client as TwitterClient

from src.config import settings
from src.intel.pain_store import PainStore  # ç—›ç‚¹ç»“æ„åŒ–å­˜å‚¨
from src.intel.utils import (
    generate_content_id,
    get_chromadb_client,
    get_output_path,
    get_today_str,
    push_to_wechat,
)
from src.utils.ai_client import get_ai_client

# ç»ˆç«¯è¾“å‡ºç¾åŒ–
console = Console()

# ç›®æ ‡äº§å“åˆ—è¡¨
TARGETS = ["DeepSeek", "ChatGPT", "Claude", "Gemini", "Cursor", "Windsurf"]

# ç—›ç‚¹å…³é”®è¯
PAIN_KEYWORDS = ["error", "fail", "broken", "slow", "stupid", "bug", "api down", "not working"]


class PainRadar:
    """ç—›ç‚¹é›·è¾¾ - æ‰«æ AI äº§å“ç”¨æˆ·æŠ±æ€¨"""

    def __init__(self):
        """åˆå§‹åŒ–ç—›ç‚¹é›·è¾¾"""
        self.pain_points: list[dict] = []  # æœ¬æ¬¡ä¼šè¯æ•è·çš„ç—›ç‚¹åˆ—è¡¨ï¼ˆåŒ…å«å…ƒæ•°æ®ï¼‰
        self._init_ai_client()  # åˆå§‹åŒ– AI å®¢æˆ·ç«¯
        self._init_pain_store()  # åˆå§‹åŒ–ç—›ç‚¹ç»“æ„åŒ–å­˜å‚¨
        self._init_chromadb()  # åˆå§‹åŒ– ChromaDBï¼ˆå‘é‡å»é‡ï¼‰

    def _init_ai_client(self):
        """åˆå§‹åŒ– AI å®¢æˆ·ç«¯"""
        if not settings.gemini.api_key:
            console.print("[red]âŒ AI API Key æœªé…ç½®[/red]")
            raise ValueError("AI API Key æœªé…ç½®")

        self.ai_client = get_ai_client()
        provider = "ç¬¬ä¸‰æ–¹èšåˆ" if settings.gemini.is_openai_compatible else "å®˜æ–¹ Gemini"
        console.print(f"[green]âœ… AI å®¢æˆ·ç«¯è¿æ¥æˆåŠŸ ({provider})[/green]")

    def _init_pain_store(self):
        """åˆå§‹åŒ–ç—›ç‚¹ç»“æ„åŒ–å­˜å‚¨ï¼ˆSQLiteï¼‰"""
        self.pain_store = PainStore()

    def _init_chromadb(self):
        """åˆå§‹åŒ– ChromaDB"""
        client = get_chromadb_client()
        self.collection = client.get_or_create_collection(name="user_pain_points")
        console.print("[green]âœ… ChromaDB æ•°æ®åº“è¿æ¥æˆåŠŸ[/green]")

    def save_pain(self, source: str, author: str, content: str, url: str = None) -> bool:
        """
        ä¿å­˜ç—›ç‚¹åˆ°æ•°æ®åº“

        åŒæ—¶å­˜å‚¨åˆ°ï¼š
        1. SQLite ç»“æ„åŒ–æ•°æ®åº“ï¼ˆæ”¯æŒæ ‡ç­¾ã€åˆ†ç±»ã€åˆå¹¶ï¼‰
        2. ChromaDB å‘é‡æ•°æ®åº“ï¼ˆç”¨äºè¯­ä¹‰å»é‡ï¼‰

        Args:
            source: æ¥æºå¹³å°
            author: ä½œè€…
            content: å†…å®¹
            url: åŸå§‹é“¾æ¥

        Returns:
            bool: æ˜¯å¦ä¿å­˜æˆåŠŸ
        """
        try:
            # 1. å­˜å‚¨åˆ° SQLite ç»“æ„åŒ–æ•°æ®åº“ï¼ˆè‡ªåŠ¨æ¨æ–­æ ‡ç­¾ã€æ£€æµ‹ç›¸ä¼¼ã€åˆå¹¶ï¼‰
            pain, is_new = self.pain_store.add_pain(
                content=content,
                source=source,
                author=author,
                original_url=url,
                auto_merge=True,  # è‡ªåŠ¨åˆå¹¶ç›¸ä¼¼ç—›ç‚¹
            )

            # 2. åŒæ—¶å­˜å‚¨åˆ° ChromaDBï¼ˆä¿æŒå‘é‡ç´¢å¼•ï¼‰
            current_time = datetime.datetime.now().isoformat()
            doc_id = generate_content_id("PAIN", content, author)

            self.collection.upsert(
                documents=[content],
                metadatas=[
                    {
                        "source": source,
                        "author": str(author),
                        "type": "pain",
                        "time": current_time,
                        "platform": pain.platform or "",
                        "category": pain.category or "",
                    }
                ],
                ids=[doc_id],
            )

            # è®°å½•åˆ°æœ¬æ¬¡ä¼šè¯åˆ—è¡¨ï¼ˆåŒ…å«ç»“æ„åŒ–æ•°æ®ï¼‰
            self.pain_points.append(
                {
                    "id": pain.id,
                    "content": content,
                    "source": source,
                    "author": author,
                    "platform": pain.platform,
                    "category": pain.category,
                    "severity": pain.severity,
                    "tags": pain.tags,
                    "frequency": pain.frequency,
                    "is_new": is_new,
                }
            )

            status = "æ–°å¢" if is_new else f"åˆå¹¶(é¢‘ç‡:{pain.frequency})"
            console.print(f"  ğŸ©¸ æ•è·ç—›ç‚¹ [{status}]: {content[:40]}...")
            return True

        except Exception as e:
            console.print(f"  [red]âš ï¸ å­˜å‚¨å¤±è´¥: {e}[/red]")
            return False

    async def scan_twitter(self) -> int:
        """
        æ‰«æ Twitter ä¸Šçš„ç”¨æˆ·æŠ±æ€¨

        Returns:
            int: æ•è·çš„ç—›ç‚¹æ•°é‡
        """
        console.print("\n[bold cyan]ğŸ¦ æ­£åœ¨æ‰«æ Twitter æœ€æ–°æ„¤æ€’å€¼...[/bold cyan]")
        client = TwitterClient(language="en-US")
        count = 0

        # ç”Ÿæˆæœç´¢è¯ç»„åˆ
        search_queries = []
        for target in TARGETS:
            for pain in PAIN_KEYWORDS:
                search_queries.append(f'"{target}" {pain}')

        # éšæœºé€‰æ‹© 5 ä¸ªæœç´¢è¯
        selected_queries = random.sample(search_queries, min(5, len(search_queries)))
        console.print(f"ğŸ¯ æœ¬æ¬¡æœç´¢è¯: {selected_queries}")

        # åŠ è½½ Twitter Cookies
        cookies_file = settings.twitter.cookies_file
        if not cookies_file.exists():
            console.print(f"[red]âŒ Twitter Cookies æ–‡ä»¶ä¸å­˜åœ¨: {cookies_file}[/red]")
            return 0

        try:
            # åŠ è½½å¹¶è½¬æ¢ cookies æ ¼å¼
            # Cookie-Editor å¯¼å‡ºæ ¼å¼: [{name, value, ...}, ...]
            # twikit 2.x æœŸæœ›æ ¼å¼: {name: value, ...}
            import json

            with open(cookies_file, encoding="utf-8") as f:
                cookies_data = json.load(f)

            # æ£€æŸ¥æ ¼å¼å¹¶è½¬æ¢
            if isinstance(cookies_data, list):
                # Cookie-Editor æ•°ç»„æ ¼å¼ â†’ å­—å…¸æ ¼å¼
                cookies_dict = {c["name"]: c["value"] for c in cookies_data if "name" in c and "value" in c}
                console.print(f"[dim]ğŸ”„ å·²è½¬æ¢ {len(cookies_dict)} ä¸ª cookies ä¸º twikit æ ¼å¼[/dim]")
                client.set_cookies(cookies_dict)
            elif isinstance(cookies_data, dict):
                # å·²ç»æ˜¯å­—å…¸æ ¼å¼ï¼Œç›´æ¥ä½¿ç”¨
                client.set_cookies(cookies_data)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„ cookies æ ¼å¼: {type(cookies_data)}")

            for query in selected_queries:
                console.print(f"  ğŸ” æœç´¢: {query}")

                try:
                    tweets = await client.search_tweet(query, product="Latest", count=3)

                    if not tweets:
                        console.print("     (æ— ç»“æœ)")
                        continue

                    for tweet in tweets:
                        text = tweet.text.replace("\n", " ")
                        user = tweet.user.name if tweet.user else "Unknown"
                        url = f"https://twitter.com/{tweet.user.screen_name}/status/{tweet.id}" if tweet.user else None
                        if self.save_pain("Twitter", user, text, url):
                            count += 1

                    await asyncio.sleep(2)  # é¿å…é™æµ

                except Exception as e:
                    console.print(f"     [yellow]âš ï¸ æœç´¢å‡ºé”™: {e}[/yellow]")

        except Exception as e:
            console.print(f"[red]âŒ Twitter ç™»å½•å¤±è´¥: {e}[/red]")

        return count

    async def scan_reddit(self) -> int:
        """
        æ‰«æ Reddit ä¸Šçš„ç”¨æˆ·ç—›ç‚¹

        Returns:
            int: æ•è·çš„ç—›ç‚¹æ•°é‡
        """
        console.print("\n[bold cyan]ğŸ”´ æ­£åœ¨æ‰«æ Reddit ç”¨æˆ·ç—›ç‚¹...[/bold cyan]")
        count = 0

        try:
            from src.intel.reddit_hunter import RedditHunter

            hunter = RedditHunter(mode="pain")
            await hunter.run()

            # å°† Reddit ç—›ç‚¹ä¿å­˜åˆ°æ•°æ®åº“
            for pain_data in hunter.get_pain_points():
                content = pain_data.get("content", "")
                author = pain_data.get("author", "Reddit User")
                url = pain_data.get("url", "")
                subreddit = pain_data.get("subreddit", "")

                if self.save_pain(f"Reddit/r/{subreddit}", author, content, url):
                    count += 1

            console.print(f"[green]âœ… Reddit é‡‡é›†å®Œæˆ: {count} æ¡ç—›ç‚¹[/green]")

        except ImportError as e:
            console.print(f"[yellow]âš ï¸ Reddit æ¨¡å—ä¸å¯ç”¨: {e}[/yellow]")

        except Exception as e:
            console.print(f"[red]âŒ Reddit é‡‡é›†å¤±è´¥: {e}[/red]")

        return count

    def analyze_pain_points(self, raw_data: str) -> str:
        """
        ä½¿ç”¨ Gemini åˆ†æç—›ç‚¹

        Args:
            raw_data: åŸå§‹ç—›ç‚¹æ•°æ®

        Returns:
            str: åˆ†ææŠ¥å‘Š
        """
        console.print("\n[bold cyan]ğŸ‘¨â€âš•ï¸ AI æ­£åœ¨è¯Šæ–­ç—›ç‚¹...[/bold cyan]")

        prompt = f"""
        # Role: AI Ecosystem Pathologist (AI ç”Ÿæ€ç—…ç†å­¦å®¶)
        # Specialization: æ“…é•¿ä»æ··ä¹±çš„ç”¨æˆ·æŠ±æ€¨ä¸­ï¼Œè§£å‰–å‡ºå¤§æ¨¡å‹çš„åº•å±‚ç¼ºé™·ï¼Œå¹¶æ„å»ºå·¥ç¨‹çº§è§£å†³æ–¹æ¡ˆã€‚

        # Workflow (è¯Šæ–­æµç¨‹)
        ## Step 1: å™ªéŸ³æçº¯ (Triage)
        åˆ†æä»¥ä¸‹æ•°æ®ï¼Œè¿‡æ»¤æƒ…ç»ªå‘æ³„ï¼Œæç‚¼å‡º **Top 3 é˜»æ–­æ€§ç—›ç‚¹ (Blockers)**ã€‚

        ## Step 2: æ·±åº¦å°¸æ£€ (Technical Autopsy)
        é’ˆå¯¹ Top 1 ç—›ç‚¹ï¼Œè¿›è¡ŒæŠ€æœ¯å±‚é¢çš„æ ¹å› åˆ†æã€‚ä½¿ç”¨ LLM åŸç†æœ¯è¯­ (å¦‚ Context Window Drift, Hallucination via Logic Gap)ã€‚

        ## Step 3: åŸºå› ç¼–è¾‘ (The Engineering Cure)
        ç¼–å†™ä¸€ä¸ª **System Prompt** æ¥è§„é¿æ­¤ç¼ºé™·ã€‚å¿…é¡»åŒ…å«ï¼šé˜²å¾¡æ€§æ€ç»´ã€ç»“æ„åŒ–è¾“å‡ºã€æ€ç»´é“¾ (CoT)ã€‚

        ## Step 4: ç”Ÿæ€ä½å—…æ¢ (The Gap Analysis)
        æ¨æ¼”ä¸€ä¸ª Micro-SaaS äº§å“å½¢æ€ã€‚

        # Raw User Data
        {raw_data}
        """

        max_retries = 5
        wait_time = 5

        for attempt in range(max_retries):
            try:
                response = self.ai_client.generate_sync(prompt)
                return response.text

            except Exception as e:
                console.print(f"[yellow]âš ï¸ è¯Šæ–­å°è¯• {attempt + 1}/{max_retries} å¤±è´¥: {e}[/yellow]")
                if attempt < max_retries - 1:
                    console.print(f"â³ ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                    wait_time += 5
                else:
                    console.print("[red]âŒ é‡è¯•è€—å°½ï¼Œæ”¾å¼ƒæ²»ç–—[/red]")

        return "âŒ è¯Šæ–­å¤±è´¥: ç½‘ç»œæˆ–APIé”™è¯¯"

    def deliver_report(self, content: str):
        """
        ç”ŸæˆæŠ¥å‘Šå¹¶æ¨é€

        Args:
            content: æŠ¥å‘Šå†…å®¹
        """
        if content.startswith("âŒ"):
            console.print(f"\n[red]ğŸš« ä»»åŠ¡ç»ˆæ­¢: {content}[/red]")
            return

        today = get_today_str()

        # ä¿å­˜ MD æŠ¥å‘Šåˆ°æ–‡ä»¶
        try:
            md_filename = f"Pain_Report_{today}.md"
            md_filepath = get_output_path(md_filename, "reports")
            md_content = f"# ğŸ’Š AI ç—›ç‚¹è¯Šæ–­æŠ¥å‘Š ({today})\n\n{content}"
            with open(md_filepath, "w", encoding="utf-8") as f:
                f.write(md_content)
            console.print(f"[green]ğŸ“ MD æŠ¥å‘Šå·²ä¿å­˜: {md_filepath}[/green]")

            # å°†æŠ¥å‘Šå†…å®¹å­˜å…¥æ•°æ®åº“ï¼ˆChromaDBï¼‰
            self._save_report_to_db(today, md_content)

        except Exception as e:
            console.print(f"[yellow]âš ï¸ MD æŠ¥å‘Šä¿å­˜å¤±è´¥: {e}[/yellow]")

        # æ¨é€åˆ°å¾®ä¿¡
        wechat_body = f"# ğŸ’Š AI ç—›ç‚¹è¯Šæ–­æŠ¥å‘Š ({today})\n\n{content}"
        push_to_wechat(title="ã€ç—›ç‚¹é›·è¾¾ã€‘è¯Šæ–­æŠ¥å‘Š", content=wechat_body)

    def _save_report_to_db(self, date: str, content: str):
        """
        å°†æŠ¥å‘Šä¿å­˜åˆ° ChromaDB æ•°æ®åº“

        Args:
            date: æŠ¥å‘Šæ—¥æœŸ
            content: æŠ¥å‘Šå†…å®¹
        """
        try:
            report_id = f"pain_report_{date}"
            self.collection.upsert(
                documents=[content],
                metadatas=[
                    {
                        "type": "pain_report",
                        "date": date,
                        "source": "pain_radar",
                        "time": datetime.datetime.now().isoformat(),
                    }
                ],
                ids=[report_id],
            )
            console.print("[green]ğŸ’¾ æŠ¥å‘Šå·²å­˜å…¥æ•°æ®åº“[/green]")
        except Exception as e:
            console.print(f"[yellow]âš ï¸ æ•°æ®åº“å­˜å‚¨å¤±è´¥: {e}[/yellow]")

    async def run(self):
        """è¿è¡Œç—›ç‚¹é›·è¾¾å®Œæ•´æµç¨‹ï¼ˆTwitter + Redditï¼‰"""
        # å¹¶è¡Œæ‰«æ Twitter å’Œ Reddit
        twitter_count = await self.scan_twitter()
        reddit_count = await self.scan_reddit()

        total_count = twitter_count + reddit_count
        console.print(f"\nğŸ“Š å…±æ•è· {total_count} ä¸ªç—›ç‚¹ (Twitter: {twitter_count}, Reddit: {reddit_count})")

        # æ‰“å°æ•°æ®åº“ç»Ÿè®¡
        self.pain_store.print_stats()

        if total_count > 0:
            # æ ¼å¼åŒ–ç—›ç‚¹æ•°æ®ç”¨äº AI åˆ†æ
            raw_pain = self._format_pains_for_analysis()
            report = self.analyze_pain_points(raw_pain)
            self.deliver_report(report)

            # æ›´æ–° AI åˆ†æç»“æœåˆ°æ•°æ®åº“
            self._update_ai_analysis(report)
        else:
            console.print("[yellow]ğŸ¤·â€â™‚ï¸ æœªæ•è·åˆ°ç—›ç‚¹ï¼Œæµç¨‹ç»“æŸ[/yellow]")

        # å…³é—­æ•°æ®åº“è¿æ¥
        self.pain_store.close()

    def _format_pains_for_analysis(self) -> str:
        """æ ¼å¼åŒ–ç—›ç‚¹æ•°æ®ä¾› AI åˆ†æ"""
        lines = []
        for i, pain in enumerate(self.pain_points, 1):
            platform = pain.get("platform", "Unknown")
            category = pain.get("category", "Unknown")
            tags = ", ".join(pain.get("tags", [])[:3])
            content = pain.get("content", "")
            lines.append(f"{i}. [{platform}][{category}] {content}")
            if tags:
                lines.append(f"   æ ‡ç­¾: {tags}")
        return "\n".join(lines)

    def _update_ai_analysis(self, report: str):
        """å°† AI åˆ†æç»“æœæ›´æ–°åˆ°æ•°æ®åº“"""
        # ä¸ºæœ¬æ¬¡æ•è·çš„æ‰€æœ‰ç—›ç‚¹æ›´æ–°åˆ†ææ‘˜è¦
        for pain in self.pain_points:
            if pain.get("is_new"):  # åªæ›´æ–°æ–°å¢çš„ç—›ç‚¹
                self.pain_store.update_ai_analysis(pain_id=pain["id"], analysis=f"æŠ¥å‘Šæ—¥æœŸ: {get_today_str()}")


async def main():
    """ä¸»å‡½æ•°å…¥å£"""
    console.print("[bold magenta]ğŸ“¡ ç—›ç‚¹é›·è¾¾ v3.0 å¯åŠ¨ (Twitter + Reddit)[/bold magenta]\n")

    try:
        radar = PainRadar()
        await radar.run()
    except KeyboardInterrupt:
        console.print("\n[yellow]âš ï¸ ç”¨æˆ·ä¸­æ–­[/yellow]")
    except Exception as e:
        console.print(f"[red]âŒ è¿è¡Œå¤±è´¥: {e}[/red]")
        raise


if __name__ == "__main__":
    asyncio.run(main())
