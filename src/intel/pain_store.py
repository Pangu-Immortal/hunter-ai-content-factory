"""
Hunter AI å†…å®¹å·¥å‚ - ç—›ç‚¹æ•°æ®å­˜å‚¨æ¨¡å—

åŠŸèƒ½ï¼š
- SQLite ç»“æ„åŒ–å­˜å‚¨æ‰€æœ‰é‡‡é›†çš„ç—›ç‚¹
- æ ‡ç­¾ç³»ç»Ÿï¼ˆäº§å“ã€ç±»å‹ã€ä¸¥é‡ç¨‹åº¦ç­‰ï¼‰
- ç›¸ä¼¼ç—›ç‚¹æ£€æµ‹ä¸åˆå¹¶
- ChromaDB å‘é‡ç›¸ä¼¼åº¦è¾…åŠ©

è¡¨ç»“æ„ï¼š
- pain_points: ç—›ç‚¹ä¸»è¡¨
- æ ‡ç­¾ä½¿ç”¨ JSON æ•°ç»„å­˜å‚¨ï¼ˆç®€åŒ–è®¾è®¡ï¼‰

ä½¿ç”¨æ–¹æ³•ï¼š
    from src.intel.pain_store import PainStore

    store = PainStore()
    store.add_pain(content="...", source="twitter", ...)
    similar = store.find_similar("...")

GitHub: https://github.com/Pangu-Immortal/hunter-ai-content-factory
Author: Pangu-Immortal
"""

import hashlib
import json
from datetime import datetime
from pathlib import Path

from rich.console import Console
from rich.table import Table
from sqlalchemy import JSON, Column, DateTime, Float, Integer, String, Text, create_engine
from sqlalchemy.orm import declarative_base, sessionmaker

from src.config import ROOT_DIR
from src.intel.utils import get_chromadb_client

# ç»ˆç«¯è¾“å‡º
console = Console()

# SQLAlchemy åŸºç±»
Base = declarative_base()

# æ•°æ®åº“è·¯å¾„
DB_PATH = ROOT_DIR / "data" / "pain_points.db"


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# æ•°æ®æ¨¡å‹å®šä¹‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class PainPointModel(Base):
    """
    ç—›ç‚¹æ•°æ®è¡¨

    å­˜å‚¨æ‰€æœ‰é‡‡é›†çš„ç”¨æˆ·ç—›ç‚¹ï¼Œæ”¯æŒæ ‡ç­¾ã€åˆ†ç±»ã€åˆå¹¶ç­‰åŠŸèƒ½
    """

    __tablename__ = "pain_points"

    # ä¸»é”®ï¼šåŸºäºå†…å®¹çš„å”¯ä¸€ ID
    id = Column(String(64), primary_key=True, comment="å”¯ä¸€IDï¼ˆå†…å®¹hashï¼‰")

    # æ ¸å¿ƒå†…å®¹
    content = Column(Text, nullable=False, comment="ç—›ç‚¹åŸæ–‡å†…å®¹")
    content_normalized = Column(Text, comment="æ ‡å‡†åŒ–åçš„å†…å®¹ï¼ˆå»é™¤å™ªéŸ³ï¼‰")

    # æ¥æºä¿¡æ¯
    source = Column(String(32), nullable=False, comment="æ¥æºå¹³å°: twitter/weibo/xiaohongshu/hackernews")
    platform = Column(String(32), comment="ç›¸å…³äº§å“: ChatGPT/Claude/DeepSeek/Geminiç­‰")
    author = Column(String(128), comment="åŸä½œè€…")
    original_url = Column(String(512), comment="åŸå§‹é“¾æ¥")

    # æ ‡ç­¾ç³»ç»Ÿï¼ˆJSON æ•°ç»„ï¼‰
    tags = Column(JSON, default=list, comment="æ ‡ç­¾åˆ—è¡¨")

    # åˆ†ç±»ä¸ä¸¥é‡ç¨‹åº¦
    category = Column(String(32), comment="é—®é¢˜åˆ†ç±»: æ€§èƒ½/å‡†ç¡®æ€§/åŠŸèƒ½/ä½“éªŒ/å®šä»·/å…¶ä»–")
    severity = Column(String(16), default="minor", comment="ä¸¥é‡ç¨‹åº¦: blocker/major/minor/enhancement")

    # AI åˆ†æç»“æœ
    ai_analysis = Column(Text, comment="AI åˆ†ææ‘˜è¦")
    ai_solution = Column(Text, comment="AI å»ºè®®çš„è§£å†³æ–¹æ¡ˆ")

    # ç»Ÿè®¡ä¿¡æ¯
    frequency = Column(Integer, default=1, comment="å‡ºç°é¢‘ç‡ï¼ˆç›¸ä¼¼ç—›ç‚¹åˆå¹¶æ—¶ç´¯åŠ ï¼‰")
    similarity_score = Column(Float, comment="ä¸ä¸»ç—›ç‚¹çš„ç›¸ä¼¼åº¦ï¼ˆåˆå¹¶æ—¶è®°å½•ï¼‰")

    # åˆå¹¶è¿½è¸ª
    merged_from = Column(JSON, default=list, comment="åˆå¹¶æ¥æºIDåˆ—è¡¨")
    is_primary = Column(Integer, default=1, comment="æ˜¯å¦ä¸ºä¸»ç—›ç‚¹ï¼ˆ1=ä¸»ï¼Œ0=å·²åˆå¹¶åˆ°å…¶ä»–ï¼‰")
    merged_to = Column(String(64), comment="åˆå¹¶åˆ°å“ªä¸ªä¸»ç—›ç‚¹ï¼ˆå¦‚æœis_primary=0ï¼‰")

    # æ—¶é—´æˆ³
    created_at = Column(DateTime, default=datetime.now, comment="é¦–æ¬¡å‘ç°æ—¶é—´")
    updated_at = Column(DateTime, default=datetime.now, onupdate=datetime.now, comment="æœ€åæ›´æ–°æ—¶é—´")

    def to_dict(self) -> dict:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "id": self.id,
            "content": self.content,
            "source": self.source,
            "platform": self.platform,
            "author": self.author,
            "tags": self.tags or [],
            "category": self.category,
            "severity": self.severity,
            "frequency": self.frequency,
            "ai_analysis": self.ai_analysis,
            "created_at": self.created_at.isoformat() if self.created_at else None,
            "updated_at": self.updated_at.isoformat() if self.updated_at else None,
        }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# æ ‡ç­¾å®šä¹‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# äº§å“æ ‡ç­¾
PRODUCT_TAGS = [
    "ChatGPT",
    "Claude",
    "Gemini",
    "DeepSeek",
    "Cursor",
    "Windsurf",
    "Copilot",
    "Perplexity",
    "Midjourney",
    "DALL-E",
    "Stable Diffusion",
    "LangChain",
    "LlamaIndex",
    "OpenAI API",
    "Anthropic API",
]

# é—®é¢˜ç±»å‹æ ‡ç­¾
CATEGORY_TAGS = {
    "æ€§èƒ½": ["slow", "timeout", "lag", "latency", "é€Ÿåº¦æ…¢", "å¡é¡¿", "è¶…æ—¶"],
    "å‡†ç¡®æ€§": ["wrong", "incorrect", "hallucination", "é”™è¯¯", "ä¸å‡†ç¡®", "èƒ¡è¯´"],
    "åŠŸèƒ½": ["missing", "not working", "broken", "åŠŸèƒ½ç¼ºå¤±", "ä¸å·¥ä½œ", "åäº†"],
    "ä½“éªŒ": ["confusing", "hard to use", "ugly", "éš¾ç”¨", "ç•Œé¢", "ä½“éªŒå·®"],
    "å®šä»·": ["expensive", "pricing", "cost", "è´µ", "æ”¶è´¹", "ä»·æ ¼"],
    "API": ["api error", "rate limit", "quota", "æ¥å£", "é™æµ", "é…é¢"],
    "ç¨³å®šæ€§": ["crash", "down", "outage", "å´©æºƒ", "å®•æœº", "ä¸ç¨³å®š"],
}

# ä¸¥é‡ç¨‹åº¦
SEVERITY_KEYWORDS = {
    "blocker": ["can't", "impossible", "completely", "totally", "å®Œå…¨ä¸èƒ½", "å½»åº•"],
    "major": ["very", "really", "seriously", "éå¸¸", "ä¸¥é‡", "å¾ˆ"],
    "minor": ["sometimes", "occasionally", "slightly", "æœ‰æ—¶", "å¶å°”", "è½»å¾®"],
}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ç—›ç‚¹å­˜å‚¨ç±»
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class PainStore:
    """
    ç—›ç‚¹æ•°æ®å­˜å‚¨

    åŠŸèƒ½ï¼š
    - SQLite ç»“æ„åŒ–å­˜å‚¨
    - ChromaDB å‘é‡ç›¸ä¼¼åº¦æ£€ç´¢
    - è‡ªåŠ¨æ ‡ç­¾æ¨æ–­
    - ç›¸ä¼¼ç—›ç‚¹åˆå¹¶
    """

    # ç›¸ä¼¼åº¦é˜ˆå€¼ï¼šé«˜äºæ­¤å€¼è®¤ä¸ºæ˜¯ç›¸ä¼¼ç—›ç‚¹
    SIMILARITY_THRESHOLD = 0.80

    def __init__(self, db_path: Path = None):
        """
        åˆå§‹åŒ–ç—›ç‚¹å­˜å‚¨

        Args:
            db_path: æ•°æ®åº“è·¯å¾„ï¼Œé»˜è®¤ä½¿ç”¨ data/pain_points.db
        """
        self.db_path = db_path or DB_PATH
        self._init_database()
        self._init_chromadb()

    def _init_database(self):
        """åˆå§‹åŒ– SQLite æ•°æ®åº“"""
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        self.db_path.parent.mkdir(parents=True, exist_ok=True)

        # åˆ›å»ºå¼•æ“å’Œä¼šè¯
        self.engine = create_engine(f"sqlite:///{self.db_path}", echo=False)
        Base.metadata.create_all(self.engine)
        session_factory = sessionmaker(bind=self.engine)
        self.session = session_factory()

        # ç»Ÿè®¡ç°æœ‰æ•°æ®
        count = self.session.query(PainPointModel).filter(PainPointModel.is_primary == 1).count()
        console.print(f"[green]âœ… ç—›ç‚¹æ•°æ®åº“è¿æ¥æˆåŠŸ (å·²å­˜å‚¨ {count} ä¸ªä¸»ç—›ç‚¹)[/green]")

    def _init_chromadb(self):
        """åˆå§‹åŒ– ChromaDB å‘é‡æ•°æ®åº“ï¼ˆç”¨äºç›¸ä¼¼åº¦æ£€ç´¢ï¼‰"""
        try:
            client = get_chromadb_client()
            self.vector_collection = client.get_or_create_collection(
                name="pain_points_vectors", metadata={"description": "ç—›ç‚¹å‘é‡å­˜å‚¨ï¼Œç”¨äºè¯­ä¹‰ç›¸ä¼¼åº¦æ£€ç´¢"}
            )
            console.print("[green]âœ… ChromaDB å‘é‡ç´¢å¼•è¿æ¥æˆåŠŸ[/green]")
        except Exception as e:
            console.print(f"[yellow]âš ï¸ ChromaDB åˆå§‹åŒ–å¤±è´¥ï¼Œå°†ç¦ç”¨ç›¸ä¼¼åº¦æ£€ç´¢: {e}[/yellow]")
            self.vector_collection = None

    def _generate_id(self, content: str, source: str) -> str:
        """ç”Ÿæˆå”¯ä¸€ID"""
        raw = f"{source}:{content}"
        return hashlib.sha256(raw.encode()).hexdigest()[:32]

    def _infer_tags(self, content: str) -> list[str]:
        """
        ä»å†…å®¹æ¨æ–­æ ‡ç­¾

        Args:
            content: ç—›ç‚¹å†…å®¹

        Returns:
            list[str]: æ¨æ–­çš„æ ‡ç­¾åˆ—è¡¨
        """
        tags = []
        content_lower = content.lower()

        # æ£€æµ‹äº§å“æ ‡ç­¾
        for product in PRODUCT_TAGS:
            if product.lower() in content_lower:
                tags.append(f"product:{product}")

        # æ£€æµ‹é—®é¢˜ç±»å‹
        for category, keywords in CATEGORY_TAGS.items():
            for keyword in keywords:
                if keyword.lower() in content_lower:
                    tags.append(f"category:{category}")
                    break

        return list(set(tags))  # å»é‡

    def _infer_category(self, content: str) -> str:
        """æ¨æ–­é—®é¢˜åˆ†ç±»"""
        content_lower = content.lower()
        for category, keywords in CATEGORY_TAGS.items():
            for keyword in keywords:
                if keyword.lower() in content_lower:
                    return category
        return "å…¶ä»–"

    def _infer_severity(self, content: str) -> str:
        """æ¨æ–­ä¸¥é‡ç¨‹åº¦"""
        content_lower = content.lower()
        for severity, keywords in SEVERITY_KEYWORDS.items():
            for keyword in keywords:
                if keyword.lower() in content_lower:
                    return severity
        return "minor"

    def _infer_platform(self, content: str) -> str:
        """æ¨æ–­ç›¸å…³äº§å“"""
        content_lower = content.lower()
        for product in PRODUCT_TAGS:
            if product.lower() in content_lower:
                return product
        return None

    def find_similar(self, content: str, threshold: float = None) -> PainPointModel | None:
        """
        æŸ¥æ‰¾ç›¸ä¼¼ç—›ç‚¹

        ä½¿ç”¨ ChromaDB å‘é‡ç›¸ä¼¼åº¦æœç´¢ï¼Œæ‰¾åˆ°æœ€ç›¸ä¼¼çš„ç°æœ‰ç—›ç‚¹

        Args:
            content: ç—›ç‚¹å†…å®¹
            threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆé»˜è®¤ä½¿ç”¨ç±»å±æ€§ï¼‰

        Returns:
            PainPointModel: æœ€ç›¸ä¼¼çš„ç—›ç‚¹ï¼Œå¦‚æœæ²¡æœ‰åˆ™è¿”å› None
        """
        if self.vector_collection is None:
            return None

        threshold = threshold or self.SIMILARITY_THRESHOLD

        try:
            # å‘é‡ç›¸ä¼¼åº¦æœç´¢
            results = self.vector_collection.query(
                query_texts=[content], n_results=1, include=["distances", "metadatas"]
            )

            if results and results["distances"] and results["distances"][0]:
                # ChromaDB è¿”å› L2 è·ç¦»ï¼Œè½¬æ¢ä¸ºç›¸ä¼¼åº¦
                distance = results["distances"][0][0]
                similarity = 1 / (1 + distance)

                if similarity >= threshold:
                    pain_id = results["ids"][0][0]
                    pain = (
                        self.session.query(PainPointModel)
                        .filter(PainPointModel.id == pain_id, PainPointModel.is_primary == 1)
                        .first()
                    )

                    if pain:
                        console.print(f"[yellow]ğŸ” å‘ç°ç›¸ä¼¼ç—›ç‚¹ (ç›¸ä¼¼åº¦: {similarity:.1%})[/yellow]")
                        return pain

            return None

        except Exception as e:
            console.print(f"[dim]âš ï¸ ç›¸ä¼¼åº¦æ£€ç´¢å¤±è´¥: {e}[/dim]")
            return None

    def add_pain(
        self,
        content: str,
        source: str,
        author: str = None,
        platform: str = None,
        original_url: str = None,
        tags: list[str] = None,
        category: str = None,
        severity: str = None,
        ai_analysis: str = None,
        auto_merge: bool = True,
    ) -> tuple[PainPointModel, bool]:
        """
        æ·»åŠ ç—›ç‚¹

        å¦‚æœå‘ç°ç›¸ä¼¼ç—›ç‚¹ï¼Œä¼šè‡ªåŠ¨åˆå¹¶ï¼ˆæ›´æ–°é¢‘ç‡ã€åˆå¹¶æ ‡ç­¾ï¼‰

        Args:
            content: ç—›ç‚¹å†…å®¹
            source: æ¥æºå¹³å°
            author: ä½œè€…
            platform: ç›¸å…³äº§å“
            original_url: åŸå§‹é“¾æ¥
            tags: æ ‡ç­¾åˆ—è¡¨
            category: é—®é¢˜åˆ†ç±»
            severity: ä¸¥é‡ç¨‹åº¦
            ai_analysis: AI åˆ†æç»“æœ
            auto_merge: æ˜¯å¦è‡ªåŠ¨åˆå¹¶ç›¸ä¼¼ç—›ç‚¹

        Returns:
            tuple[PainPointModel, bool]: (ç—›ç‚¹å¯¹è±¡, æ˜¯å¦ä¸ºæ–°å¢)
        """
        # è‡ªåŠ¨æ¨æ–­ç¼ºå¤±å­—æ®µ
        inferred_tags = self._infer_tags(content)
        final_tags = list(set((tags or []) + inferred_tags))

        platform = platform or self._infer_platform(content)
        category = category or self._infer_category(content)
        severity = severity or self._infer_severity(content)

        # æ£€æŸ¥ç›¸ä¼¼ç—›ç‚¹
        if auto_merge:
            similar = self.find_similar(content)
            if similar:
                # åˆå¹¶åˆ°ç°æœ‰ç—›ç‚¹
                return self._merge_pain(similar, content, source, author, final_tags, ai_analysis)

        # åˆ›å»ºæ–°ç—›ç‚¹
        pain_id = self._generate_id(content, source)

        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ï¼ˆç²¾ç¡®åŒ¹é…ï¼‰
        existing = self.session.query(PainPointModel).filter(PainPointModel.id == pain_id).first()
        if existing:
            # æ›´æ–°é¢‘ç‡
            existing.frequency += 1
            existing.updated_at = datetime.now()
            # åˆå¹¶æ ‡ç­¾
            existing_tags = existing.tags or []
            existing.tags = list(set(existing_tags + final_tags))
            self.session.commit()
            console.print(f"[cyan]ğŸ“ æ›´æ–°å·²æœ‰ç—›ç‚¹ (é¢‘ç‡: {existing.frequency})[/cyan]")
            return existing, False

        # æ–°å»ºç—›ç‚¹
        pain = PainPointModel(
            id=pain_id,
            content=content,
            content_normalized=content.lower().strip(),
            source=source,
            platform=platform,
            author=author,
            original_url=original_url,
            tags=final_tags,
            category=category,
            severity=severity,
            ai_analysis=ai_analysis,
            frequency=1,
            is_primary=1,
            merged_from=[],
            created_at=datetime.now(),
            updated_at=datetime.now(),
        )

        self.session.add(pain)
        self.session.commit()

        # åŒæ­¥åˆ°å‘é‡æ•°æ®åº“
        self._add_to_vector_db(pain)

        console.print(f"[green]ğŸ’¾ æ–°å¢ç—›ç‚¹: {content[:40]}... [æ ‡ç­¾: {', '.join(final_tags[:3])}][/green]")
        return pain, True

    def _merge_pain(
        self,
        primary: PainPointModel,
        new_content: str,
        new_source: str,
        new_author: str,
        new_tags: list[str],
        new_analysis: str = None,
    ) -> tuple[PainPointModel, bool]:
        """
        åˆå¹¶ç—›ç‚¹åˆ°ä¸»ç—›ç‚¹

        Args:
            primary: ä¸»ç—›ç‚¹
            new_content: æ–°å†…å®¹
            new_source: æ–°æ¥æº
            new_author: æ–°ä½œè€…
            new_tags: æ–°æ ‡ç­¾
            new_analysis: æ–°åˆ†æ

        Returns:
            tuple[PainPointModel, bool]: (ä¸»ç—›ç‚¹, Falseè¡¨ç¤ºåˆå¹¶è€Œéæ–°å¢)
        """
        # æ›´æ–°ä¸»ç—›ç‚¹
        primary.frequency += 1
        primary.updated_at = datetime.now()

        # åˆå¹¶æ ‡ç­¾
        existing_tags = primary.tags or []
        primary.tags = list(set(existing_tags + new_tags))

        # è®°å½•åˆå¹¶æ¥æº
        new_id = self._generate_id(new_content, new_source)
        merged_from = primary.merged_from or []
        if new_id not in merged_from:
            merged_from.append(new_id)
            primary.merged_from = merged_from

        # æ›´æ–° AI åˆ†æï¼ˆå¦‚æœæ–°çš„æ›´è¯¦ç»†ï¼‰
        if new_analysis and (not primary.ai_analysis or len(new_analysis) > len(primary.ai_analysis)):
            primary.ai_analysis = new_analysis

        self.session.commit()

        console.print(f"[cyan]ğŸ”— åˆå¹¶åˆ°ä¸»ç—›ç‚¹ (é¢‘ç‡: {primary.frequency}, æ ‡ç­¾: {len(primary.tags)})[/cyan]")
        return primary, False

    def _add_to_vector_db(self, pain: PainPointModel):
        """æ·»åŠ åˆ°å‘é‡æ•°æ®åº“"""
        if self.vector_collection is None:
            return

        try:
            self.vector_collection.upsert(
                ids=[pain.id],
                documents=[pain.content],
                metadatas=[
                    {
                        "source": pain.source,
                        "platform": pain.platform or "",
                        "category": pain.category or "",
                        "severity": pain.severity or "",
                    }
                ],
            )
        except Exception as e:
            console.print(f"[dim]âš ï¸ å‘é‡å­˜å‚¨å¤±è´¥: {e}[/dim]")

    def update_ai_analysis(self, pain_id: str, analysis: str, solution: str = None):
        """
        æ›´æ–° AI åˆ†æç»“æœ

        Args:
            pain_id: ç—›ç‚¹ ID
            analysis: AI åˆ†ææ‘˜è¦
            solution: AI å»ºè®®çš„è§£å†³æ–¹æ¡ˆ
        """
        pain = self.session.query(PainPointModel).filter(PainPointModel.id == pain_id).first()
        if pain:
            pain.ai_analysis = analysis
            if solution:
                pain.ai_solution = solution
            pain.updated_at = datetime.now()
            self.session.commit()

    def get_by_platform(self, platform: str, limit: int = 50) -> list[PainPointModel]:
        """æŒ‰äº§å“è·å–ç—›ç‚¹"""
        return (
            self.session.query(PainPointModel)
            .filter(PainPointModel.platform == platform, PainPointModel.is_primary == 1)
            .order_by(PainPointModel.frequency.desc())
            .limit(limit)
            .all()
        )

    def get_by_category(self, category: str, limit: int = 50) -> list[PainPointModel]:
        """æŒ‰åˆ†ç±»è·å–ç—›ç‚¹"""
        return (
            self.session.query(PainPointModel)
            .filter(PainPointModel.category == category, PainPointModel.is_primary == 1)
            .order_by(PainPointModel.frequency.desc())
            .limit(limit)
            .all()
        )

    def get_top_pains(self, limit: int = 20) -> list[PainPointModel]:
        """è·å–é«˜é¢‘ç—›ç‚¹"""
        return (
            self.session.query(PainPointModel)
            .filter(PainPointModel.is_primary == 1)
            .order_by(PainPointModel.frequency.desc())
            .limit(limit)
            .all()
        )

    def get_recent_pains(self, days: int = 7, limit: int = 50) -> list[PainPointModel]:
        """è·å–æœ€è¿‘çš„ç—›ç‚¹"""
        from datetime import timedelta

        cutoff = datetime.now() - timedelta(days=days)
        return (
            self.session.query(PainPointModel)
            .filter(PainPointModel.created_at >= cutoff, PainPointModel.is_primary == 1)
            .order_by(PainPointModel.created_at.desc())
            .limit(limit)
            .all()
        )

    def get_stats(self) -> dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        total = self.session.query(PainPointModel).filter(PainPointModel.is_primary == 1).count()

        # æŒ‰äº§å“ç»Ÿè®¡
        from sqlalchemy import func

        platform_stats = (
            self.session.query(
                PainPointModel.platform, func.count(PainPointModel.id), func.sum(PainPointModel.frequency)
            )
            .filter(PainPointModel.is_primary == 1, PainPointModel.platform.isnot(None))
            .group_by(PainPointModel.platform)
            .all()
        )

        # æŒ‰åˆ†ç±»ç»Ÿè®¡
        category_stats = (
            self.session.query(
                PainPointModel.category, func.count(PainPointModel.id), func.sum(PainPointModel.frequency)
            )
            .filter(PainPointModel.is_primary == 1, PainPointModel.category.isnot(None))
            .group_by(PainPointModel.category)
            .all()
        )

        # æŒ‰ä¸¥é‡ç¨‹åº¦ç»Ÿè®¡
        severity_stats = (
            self.session.query(PainPointModel.severity, func.count(PainPointModel.id))
            .filter(PainPointModel.is_primary == 1)
            .group_by(PainPointModel.severity)
            .all()
        )

        return {
            "total_pains": total,
            "by_platform": {p: {"count": c, "frequency": f} for p, c, f in platform_stats if p},
            "by_category": {c: {"count": cnt, "frequency": f} for c, cnt, f in category_stats if c},
            "by_severity": {s: cnt for s, cnt in severity_stats if s},
        }

    def print_stats(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.get_stats()

        console.print("\n[bold magenta]ğŸ“Š ç—›ç‚¹æ•°æ®åº“ç»Ÿè®¡[/bold magenta]\n")
        console.print(f"æ€»ç—›ç‚¹æ•°: [bold]{stats['total_pains']}[/bold]\n")

        # æŒ‰äº§å“ç»Ÿè®¡
        if stats["by_platform"]:
            table = Table(title="æŒ‰äº§å“ç»Ÿè®¡")
            table.add_column("äº§å“", style="cyan")
            table.add_column("ç—›ç‚¹æ•°", justify="right")
            table.add_column("æ€»é¢‘ç‡", justify="right")

            for platform, data in sorted(stats["by_platform"].items(), key=lambda x: x[1]["frequency"], reverse=True):
                table.add_row(platform, str(data["count"]), str(data["frequency"]))

            console.print(table)

        # æŒ‰åˆ†ç±»ç»Ÿè®¡
        if stats["by_category"]:
            table = Table(title="æŒ‰é—®é¢˜ç±»å‹ç»Ÿè®¡")
            table.add_column("ç±»å‹", style="yellow")
            table.add_column("ç—›ç‚¹æ•°", justify="right")
            table.add_column("æ€»é¢‘ç‡", justify="right")

            for category, data in sorted(stats["by_category"].items(), key=lambda x: x[1]["frequency"], reverse=True):
                table.add_row(category, str(data["count"]), str(data["frequency"]))

            console.print(table)

    def export_to_json(self, output_path: Path = None) -> Path:
        """å¯¼å‡ºä¸º JSON"""
        output_path = output_path or (ROOT_DIR / "output" / "pain_points_export.json")
        output_path.parent.mkdir(parents=True, exist_ok=True)

        pains = self.session.query(PainPointModel).filter(PainPointModel.is_primary == 1).all()
        data = {
            "exported_at": datetime.now().isoformat(),
            "total": len(pains),
            "pain_points": [p.to_dict() for p in pains],
        }

        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        console.print(f"[green]ğŸ“¦ å·²å¯¼å‡º {len(pains)} ä¸ªç—›ç‚¹åˆ°: {output_path}[/green]")
        return output_path

    def close(self):
        """å…³é—­æ•°æ®åº“è¿æ¥"""
        self.session.close()


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# æµ‹è¯•å…¥å£
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


def main():
    """æµ‹è¯•ç—›ç‚¹å­˜å‚¨"""
    console.print("[bold magenta]ğŸ§ª ç—›ç‚¹å­˜å‚¨æ¨¡å—æµ‹è¯•[/bold magenta]\n")

    store = PainStore()

    # æµ‹è¯•æ·»åŠ ç—›ç‚¹
    test_pains = [
        {
            "content": "ChatGPT is so slow today, I can't even complete a simple query",
            "source": "twitter",
            "author": "user123",
        },
        {
            "content": "Claude keeps giving me wrong answers about Python code",
            "source": "twitter",
            "author": "dev456",
        },
        {
            "content": "ChatGPT response time is terrible, taking forever to load",  # åº”è¯¥ä¸ç¬¬ä¸€ä¸ªåˆå¹¶
            "source": "twitter",
            "author": "user789",
        },
        {
            "content": "DeepSeek API is down again, third time this week",
            "source": "hackernews",
            "author": "techie",
        },
    ]

    console.print("[bold]1. æ·»åŠ æµ‹è¯•ç—›ç‚¹[/bold]\n")
    for pain_data in test_pains:
        store.add_pain(**pain_data)
        console.print("")

    console.print("\n[bold]2. æŸ¥çœ‹ç»Ÿè®¡[/bold]")
    store.print_stats()

    console.print("\n[bold]3. è·å–é«˜é¢‘ç—›ç‚¹[/bold]")
    top_pains = store.get_top_pains(5)
    for p in top_pains:
        console.print(f"  [{p.frequency}æ¬¡] {p.content[:50]}...")

    store.close()


if __name__ == "__main__":
    main()
